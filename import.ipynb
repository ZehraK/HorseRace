{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f30b5b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fde4a",
   "metadata": {},
   "source": [
    "Run the following code on terminal:\n",
    "\n",
    "/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip\n",
    "\n",
    "/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install pandas <br>\n",
    "/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install numpy <br>\n",
    "/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install pd <br>\n",
    "pip3 install beautifulsoup4 <br>\n",
    "pip3 install selenium <br>\n",
    "pip install webdriver-manager  <br>\n",
    "sudo pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a98f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import numpy as np\n",
    "import re #regex\n",
    "import lxml #use html.parser instead / no dependency issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ea69a",
   "metadata": {},
   "source": [
    "# Scrape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4b365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Load the web page\n",
    "url = \"https://www.tjk.org/TR/YarisSever/Info/Page/GunlukYarisProgrami\"\n",
    "driver.get(url)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "data2 = []\n",
    "race_metadata = []\n",
    "ids = []\n",
    "all_data = []\n",
    "\n",
    "#### SELECT A DATE RANGE \n",
    "#Data is availabe after 01/01/2018\n",
    "\n",
    "date_generated = pd.date_range(end = datetime.today(), periods = 5) #periods = 3 can use instead of start start='1/1/2018'\n",
    "date_range = date_generated.strftime(\"%d/%m/%y\").tolist()\n",
    "\n",
    "for date in date_range:\n",
    "\n",
    "    elem = driver.find_element_by_name('QueryParameter_Tarih')\n",
    "    elem.clear()\n",
    "    elem.send_keys(date)\n",
    "    time.sleep(20) #MAYBE WAIT LESS?? Don't wait less. In 10 s: Sayfayı yüklemeden şehirleri aldığı için\n",
    "    #saydfa yüklendiğinde şehir adları eşleşmediğinde unavailable path hatası verdi(09/02/2021)\n",
    "\n",
    "    \n",
    "    ## LOOP THROUGH THE ALL TABS\n",
    "    \n",
    "    ids.clear()\n",
    "    # For loop that iterates over all the <li> tags\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    for h in soup.findAll('li'):\n",
    "\n",
    "        # looking for anchor tag inside the <li>tag\n",
    "        a = h.find('a')\n",
    "        try:\n",
    "\n",
    "            # looking for href inside anchor tag\n",
    "            if 'data-sehir-id' in a.attrs:\n",
    "\n",
    "                # storing the value of href in a separate \n",
    "                # variable\n",
    "                city_id = a.get('id')\n",
    "\n",
    "                # appending the url to the output list\n",
    "                ids.append(city_id)\n",
    "\n",
    "        # if the list does not has a anchor tag or an anchor \n",
    "        # tag does not has a href params we pass\n",
    "        except:\n",
    "            pass   \n",
    "        \n",
    "    for city in ids: \n",
    "    \n",
    "        elem1= driver.find_element_by_xpath(\"//ul[@class='gunluk-tabs']\")\n",
    "\n",
    "        xpath = \"//*[@id='\"\n",
    "        xpath += str(city)\n",
    "        xpath += \"']\"\n",
    "        driver.find_element_by_xpath(xpath).click()\n",
    "        time.sleep(10)\n",
    "    \n",
    "    \n",
    "    ## COLLECT ALL THE RACES METADATA\n",
    "\n",
    "        # find all elements inside a div element of class col-lg-10\n",
    "        selector = 'div.race-details > *'\n",
    "\n",
    "        # find elements that contain the data we want\n",
    "        found = soup.select(selector)\n",
    "\n",
    "        # Extract data from the found elements\n",
    "        data = [x.text.split(';')[-1].strip() for x in found]\n",
    "\n",
    "        #remove unnecessary characters and spaces\n",
    "        for element in data:\n",
    "            data2.append(\" \".join(element.split()))\n",
    "        N = 2\n",
    "        # Consecutive N concatenation in String list\n",
    "        # using format() + zip() + iter() + list comprehension\n",
    "        temp = '{} ' * N \n",
    "        race_metadata_list = [temp.format(*ele) for ele in zip(*[iter(data2)] * N)]\n",
    "        race_metadata.extend(race_metadata_list)\n",
    "    \n",
    "        \n",
    "     ## COLLECT RUN/RACE DETAILED DATA/RESULTS\n",
    "            \n",
    "        tables = soup.find_all('table')\n",
    "        dfs = pd.read_html(str(tables))\n",
    "        \n",
    "    \n",
    "     ## PİST METADATA // Check out sentetik again in 09.06.2021, there was a problem but it may be related to \n",
    "        #the set_index function\n",
    "        \n",
    "        sentetik = soup.find_all(\"span\", {\"class\": \"raceWeatherBrown\"})\n",
    "        sentetik = [x.text.split(' ')[-1].strip() for x in sentetik]\n",
    "        \n",
    "        çim = soup.find_all(\"span\", {\"class\": \"raceWeatherGreen\"})\n",
    "        çim = sentetik = [x.text.split(' ')[-1].strip() for x in çim]\n",
    "        \n",
    "        conditions = çim+sentetik\n",
    "        \n",
    "        str_conditions = \";\".join(conditions)\n",
    "            \n",
    "     ## COMBINE ALL_DATA \n",
    "    \n",
    "        for x, df in enumerate(dfs):\n",
    "            df = dfs[x]\n",
    "            df['tarih']= date\n",
    "            df['lokasyon']= city\n",
    "            df['metadata'] = race_metadata_list[x]\n",
    "            df['race-conditions'] =  str_conditions\n",
    "            \n",
    "            #For sequences, (strings, lists, tuples), use the fact that empty sequences are false\n",
    "            all_data.append(df)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64346e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4316826",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc37e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data) #number of races/ extracted race tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[5].tarih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each race, assign a new column to indicate S/F Horse \n",
    " #For each race, generate a unique id\n",
    "##DONT RUN\n",
    "#A Unique ID\n",
    "#for df in all_data:\n",
    " #   for x in df:\n",
    "  #      df[x] = \"x\"\n",
    "        \n",
    "#Winning Type\n",
    "#if En iyi D. & includes (1) in AGF = Favori, else Sürpriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9b981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat(all_data)\n",
    "df.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac49b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metadata'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with split value columns\n",
    "new = df[\"metadata\"].str.split(\",\", n = 5, expand = True)\n",
    "new #length varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb632fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame({'race' : []})\n",
    "\n",
    "metadata[\"race\"] = new[0]\n",
    "metadata[\"group\"] = new[1]\n",
    "conditions = [\n",
    "        (pd.notnull(new[5]) & pd.notnull(new[4])),\n",
    "        (pd.isnull(new[5]) & pd.notnull(new[4])),\n",
    "        (pd.isnull(new[5]) & pd.isnull(new[4]))]\n",
    "\n",
    "choices_kg = [new[2]+new[3], new[2], None]\n",
    "metadata['kg'] = np.select(conditions, choices_kg, default=None)\n",
    "\n",
    "choices_eid = [new[5], new[4], new[3]]\n",
    "metadata['eid'] = np.select(conditions, choices_eid, default=None) #Koşunun EİD'si/ metadata yarışla alakalı\n",
    "\n",
    "choices_pist = [new[4], new[3], new[2]]\n",
    "metadata['pist'] = np.select(conditions, choices_pist, default=None)\n",
    "\n",
    "metadata.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append metadata to df \n",
    "df_1 = pd.concat([df, metadata], axis=1)\n",
    "df_1.head()\n",
    "#DF1 includes metadata splitted \n",
    "#DF2 includes metadata extended by AGF/ratio/F/S; hour, race no, tür, kg by numeric, eid by numeric, mesafe and pist türü\n",
    "\n",
    "\n",
    "#TOBE MERGED WITH JOKEY FEATURES; SAHIP & ANTRENOR FEATURES; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_1['pist'].str.extract(r\"(\\d{4})\", expand=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd04337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGEX OPS\n",
    "\n",
    "#Extract time from HH/MM format\n",
    "df['Time']=df_1['race'].str.extract(r\"(([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9])\", expand=True)[0]\n",
    "#Extract Tür after HH/MM format\n",
    " \n",
    "#Extract n_koşu before HH/MM format\n",
    "df['gunluk_kosu_no']=df_1['race'].str.extract(r\"(^.*?(?=:([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9]))\")[0]\n",
    "#Pist mesafe & type\n",
    "df['Mesafe']=df_1['pist'].str.extract(r\"(\\d{4})\", expand=True)[0]\n",
    "#extract eid as numeric\n",
    "\n",
    "#extract kg as numeric\n",
    "\n",
    "#orijin split anne-baba by \"-\" till /\n",
    "\n",
    "#orijin afte r / lokasyon/doğum yeri belki?\n",
    "\n",
    "#at yaşı numeric\n",
    "\n",
    "#at yaşı #y sonrası\n",
    "\n",
    "#siklet +? eklenen puan ayrı yeni bir sütun\n",
    "\n",
    "#KG as binary may be better like 58=1 59=1, extract digits by regex\n",
    "#Seperate anne-baba \n",
    "#split yaş ve grup\n",
    "\n",
    "#AGF Favourite/Surprise (favourite can be used as a feature while suprise can be calculated maybe?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Kosu_Türü']=df['n_kosu'].str.extract(r\"?<=([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9].*\", expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0da47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[\"pist_1\", \"mesafe\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "###EXPORT DATASET TO CSV\n",
    "all_data.to_csv('yaris_detayli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940f6cd",
   "metadata": {},
   "source": [
    "## Keep running the code / try again after sleep\n",
    "\n",
    "'NoSuchElementException: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id='Şanlıurfa']\"}'\n",
    "\n",
    "```python\n",
    "import time\n",
    " \n",
    "def do_work():\n",
    "    # whatever you're doing\n",
    " \n",
    "while True:\n",
    "    try:\n",
    "        do_work()\n",
    "    except:\n",
    "        time.sleep(10)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
